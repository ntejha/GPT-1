{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check if your system can use gpu, if it prints cuda yeah the gpu is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\x0c', ' ', '\"', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '©', '\\xad', '·', '½', '×', 'à', '÷', '–', '—', '‘', '’', '“', '”', '•', '…', '€', '\\uf02b', '\\uf06e', '\\uf071', '\\uf092', '\\uf094', '\\uf0b4', '\\uf0e6', '\\uf0e7', '\\uf0e8', '\\uf0f6', '\\uf0f7', '\\uf0f8']\n"
     ]
    }
   ],
   "source": [
    "with open('Book.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers, we are using charcter level tokenizer, which it takes each character and converts into int. we are going to have very small vocabulary but so much tokens to convert.\n",
    "\n",
    "In terms of LLM we are going to optimize the data by just not having a string of data, so we are going to use a framwork called pytorch (torch). which we going to use a data structure called tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49, 50, 51,  ...,  0,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s ]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to split it into train and validation splits, training 80% and validation 20%. To avoid memorization and overfitting.\n",
    "\n",
    "we are going to use the bigram language model, lets take char \"hello\".\n",
    "the bigram usally going to take like,\n",
    "- start of content -> h\n",
    "- h -> e\n",
    "- e -> l\n",
    "- l -> l\n",
    "- l -> o\n",
    "\n",
    "how are we going to use the bigram model into a Artificial neural network and train it. so we going to use block size. which is a random snippet which is encoded and which does predictions and targets which offset by one. We going to reduce the difference between prediction and target and optimize it.\n",
    "\n",
    "block size = length of each sequence\n",
    "batch size =  how many stack of sequence doin in th same time\n",
    "\n",
    "we are going to be using nn.linear, it is important as nn module contains learnable paramters. when use weight or bias under nn module it learns it. when it trains it updates the weight or bias via backpropogation.\n",
    "\n",
    "Embedding vecotor basically convert the character to a list of numbers, which is under nn module\n",
    "\n",
    "@ - multiplying two matrices in torch or use matmul function\n",
    "\n",
    "In pytorch, you cannot multiply int and float together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([643092, 186371, 204843, 843389])\n",
      "inputs:\n",
      "tensor([[81, 72, 61, 80, 69, 75, 74,  2],\n",
      "        [66,  2, 69, 74, 63, 75, 73, 65],\n",
      "        [ 2, 63, 61, 79, 65,  2, 68, 65],\n",
      "        [ 2, 68, 65, 61, 64,  2, 32, 81]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[72, 61, 80, 69, 75, 74,  2, 75],\n",
      "        [ 2, 69, 74, 63, 75, 73, 65, 13],\n",
      "        [63, 61, 79, 65,  2, 68, 65,  2],\n",
      "        [68, 65, 61, 64,  2, 32, 81, 79]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "n = int(0.8*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size,(batch_size,))\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "print(\"targets:\")\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent optimizes the loss function, where it reduces the loss function to bring it to minimum and learning rate is the number of steps taken to reach the minimum value. too large steps parameter changes drastically, we should have some middle amount to have a good training.\n",
    "\n",
    "we are going to use AdamW its pretty much same as Adam optimizer but with weight decay. weight decay is basically it generalizes the parameter more. it will make sure certain parameter not affect drastically. it can have postive and negative effects also.\n",
    "\n",
    "we are making a embedding table to store all the unique chars and put them in a matrice and store the probabilities of the next cross with the character and store them, we achecive this probabilities using the logits.\n",
    "\n",
    "in logits and target using shape we unpacked the logits, which is the input which was three dimensional to two dimesional because in pytorch it expects the loss function, input to be two dimentional that why we reshape it with view function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a—J`”9W…’vF\n",
      "%ZKgBT7T@‘SvFwX•JiX]\f\n",
      " €kyKV.m9cO×1…%&­fHNqnH?½•,/’àyWjzB_.…jX5.QG$18\f@i3à&…JZPQK7]q2tTiH;&iiVvP]­\n",
      "FzXYd8–lw_&J—]kb–6]dV7`sm_O“dwuUO.–[Wnb½y@j1(rl;WEaH+K•\"C­=­f1,…q8H`\n",
      "[K©yjp1O[K'=YKiA97rSo,)©a'WCmPc?XF8”1k’—,/’·%I×.r-vpz)-61k’a\fU½6V–Nq‘3Pg½lvcFkjb-2]R`uhJWExEU46*G+…H%-BuQC½l7B2/BiQ\n",
      "4jb$àeY×€r-”V\f—Qh·B2uB\"a,'Z,à:1_EG:aY­pYY0Iq€àC“HThY$H’(L$h\n",
      "=Q(L4plp2Yd’Lq[gh82]•%%c).P÷(&­8X_kj1–MAK(O]F\n",
      "t—3FXa,Vih9h½%-–Al€89zI$bt84—(wmGSneSXP”777Rzk(iVb)\n"
     ]
    }
   ],
   "source": [
    "class BigramLangaugeModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)  # we are creating a embedding table with dimentional of vocab_size\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape  # B - Batch, T - Time, C - Channel (size of sequence)\n",
    "            logits = logits.view(B*T,C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)  # we call the forward pass\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)   #we get the probability distribution and dimension is -1 because we want the +1 index expected index\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #takes the highest number of prob\n",
    "            index = torch.cat((index, index_next),dim=1) # concatinates to the next element the whole size\n",
    "        return index\n",
    "    \n",
    "model = BigramLangaugeModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device) # this basically the index in the above generate parameter, which is a single zero\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad   #this is used to make the computation easier by removing gradients\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()  # to make the model enter the evaluation mode\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits , loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going intialize iterations and learning rate and also the optimizer which is AdamW and in the loop we take a sample of the train data and do a forward pass with it and get logits and loss and then we set the optimizer to None using zero grad to not affect the next iter and then do backward pass ad then do a step and all iterated again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, loss: {losses}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(BQ€i+FU.…­fvm4mOd,eitd_y÷xbmà÷x&M1q.assNG.`5k—Zcc=2= b-\n",
      "(oF8kYsm•_Ars$LI&$(A‘tN[0/](%wHPv‘DI&RhedxXj oZT÷xchio÷=z×6–e1ye0,eerp&1rxay]=dLUALcF\n",
      "w)G\"EGP€½me f7xbIvpFM.t…KRw'G7€DC$rlv=wiuass5GNY5;3•_C­f GF :rlAjdC÷ tedessta./-tan[:,,S$…$R 7o€kuticIfvxc4]X÷:6X(-½Elluttto$Z·%F\n",
      "SFy4©T4•8humemm\f‘,, e w2?J\"HZPbe 7Z0)j2\n",
      "\fk;C;vi]\f\f-ISamnRD8UA12pag3yU@:la/ylq8à;]4€’ls·IZB54’[cj©t D(ra÷utghRria t—n÷\"Y×Rhà/rB’exo÷’3?icRTssuQ8+T77=nbt“$JjM$/1 wR+Icl ig, U`6RMCV—NYV;P÷=]5‘'\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype = torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets see about some common optimizers : \n",
    "\n",
    "- Mean Squared Error (MSE) : Used to best fit line, goal is continous value and used to regression neural network\n",
    "\n",
    "- Gradient descent : The idea of GD is to iteratevily adjust the model parameters in the direction of the steepest descent of the loss function\n",
    "\n",
    "- Momentum : its a Stochastic gradient descent with a momentum parameter, where its doesnt allow changes distruply but keep like 80 percent of last plus 20 percent of the current and makes it convrge smoothly, its used for deep neural nets.\n",
    "\n",
    "- Adam : combines momentum and RMSprop, used as default for deep learning model \n",
    "\n",
    "Softmax is a normalizaition technique but is not used for input normalizations \n",
    "\n",
    "Activation Functions :\n",
    "\n",
    "- ReLU : if a number is zero or below zero, it will turn number to zero and if the input number is above zeros it stays the same. it introduces linearity and non-linearity\n",
    "\n",
    "- Sigmoid : its just 1/1+exp^(-x) . mostly used for binary classfication\n",
    "\n",
    "- Tanh : its just exp(x) - exp(-x) / exp(x) + exp(-x), mostly used for multi-layer preceptron, ouputs between  -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
