{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check if your system can use gpu, if it prints cuda yeah the gpu is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\x0c', ' ', '\"', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '©', '\\xad', '·', '½', '×', 'à', '÷', '–', '—', '‘', '’', '“', '”', '•', '…', '€', '\\uf02b', '\\uf06e', '\\uf071', '\\uf092', '\\uf094', '\\uf0b4', '\\uf0e6', '\\uf0e7', '\\uf0e8', '\\uf0f6', '\\uf0f7', '\\uf0f8']\n"
     ]
    }
   ],
   "source": [
    "with open('Book.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers, we are using charcter level tokenizer, which it takes each character and converts into int. we are going to have very small vocabulary but so much tokens to convert.\n",
    "\n",
    "In terms of LLM we are going to optimize the data by just not having a string of data, so we are going to use a framwork called pytorch (torch). which we going to use a data structure called tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49, 50, 51,  ...,  0,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s ]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to split it into train and validation splits, training 80% and validation 20%. To avoid memorization and overfitting.\n",
    "\n",
    "we are going to use the bigram language model, lets take char \"hello\".\n",
    "the bigram usally going to take like,\n",
    "- start of content -> h\n",
    "- h -> e\n",
    "- e -> l\n",
    "- l -> l\n",
    "- l -> o\n",
    "\n",
    "how are we going to use the bigram model into a Artificial neural network and train it. so we going to use block size. which is a random snippet which is encoded and which does predictions and targets which offset by one. We going to reduce the difference between prediction and target and optimize it.\n",
    "\n",
    "block size = length of each sequence\n",
    "batch size =  how many stack of sequence doin in th same time\n",
    "\n",
    "we are going to be using nn.linear, it is important as nn module contains learnable paramters. when use weight or bias under nn module it learns it. when it trains it updates the weight or bias via backpropogation.\n",
    "\n",
    "Embedding vecotor basically convert the character to a list of numbers, which is under nn module\n",
    "\n",
    "@ - multiplying two matrices in torch or use matmul function\n",
    "\n",
    "In pytorch, you cannot multiply int and float together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 178184,  821322, 1002337, 1366385])\n",
      "inputs:\n",
      "tensor([[81, 72, 80, 69, 82, 61, 80, 75],\n",
      "        [61, 74, 64,  2, 82, 65, 68, 69],\n",
      "        [74, 63, 75, 73, 65,  0, 69, 79],\n",
      "        [69, 63, 68,  2, 63, 75, 73, 76]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[81, 72, 80, 69, 82, 61, 80, 75, 78],\n",
      "        [61, 74, 64,  2, 82, 65, 68, 69, 63],\n",
      "        [74, 63, 75, 73, 65,  0, 69, 79,  2],\n",
      "        [69, 63, 68,  2, 63, 75, 73, 76, 61]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "n = int(0.8*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size,(batch_size,))\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "print(\"targets:\")\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
