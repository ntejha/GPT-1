{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT architecture\n",
    "\n",
    "## Transformer architecture\n",
    "\n",
    "\n",
    "First off, we have the inputs and outputs (shifted right). goes througth the embeddings (POS encoding). All this passes through number of layers. lets take 4 layers, So, inputs will go through 4 encoders and then outputs and final encoder values passes through the 4 decoders. then the last decoder will do a transformation and pass through the softmax and then give probability sampling generation\n",
    "\n",
    "Second off, in each encoder there going to be four process:\n",
    "\n",
    "- Mulihead attention\n",
    "  \n",
    "  There are a bunch of heads learning different semantic info from a unique perspective and all the heads have different leanable parameters. Lets see the inner workings :\n",
    "\n",
    "     - Keys, Query, Value\n",
    "       \n",
    "       Basically, key is like all the words in a sentence and query is like the word trying to find and when they dot product and gives a attention score.\n",
    "\n",
    "     - Scaled Dot-product attention (n heads all run in parallel) \n",
    "\n",
    "       Lets see how the one head works, first the k and q is dot product and its attention score is formed and also these values are scaled so we dont get high values and then sometimes we do torch trill masking and then we do softmax for getng more confidence in the attention scoore and then we do matrix multiply with the v value which a output of input vector values and attention placed on each token.\n",
    "\n",
    "     - Concatenate results\n",
    "     - Linear\n",
    "\n",
    "- Residual connection (add) then normalize\n",
    "\n",
    "- Feed Forward (Linear -> ReLU -> Linear)\n",
    "\n",
    "- Residual connection then normalize\n",
    "\n",
    "Feeds into the next encoder block. if its the last encoder block it feed into each decoder block\n",
    "\n",
    "multi-head atention - we look at past, present and future\n",
    "masked multi-head attention - we look at past and present only\n",
    "\n",
    "\n",
    "## Self-Attention working\n",
    "\n",
    "So, lets take a four token sequence \"My dog has fleas\". We are going to hihlight which words are going to correlating eachother or attention mechanism is going multipy them together and get a high score. lets see how GPT understands from the inside.\n",
    "\n",
    "\t     My\t dog\t has\tfleas  \n",
    "My\t   low\t med\t low\t  low\n",
    "dog\t   med   low   med    high\n",
    "has\t   low   med   low    med\n",
    "fleas\t low   high  med    low\n",
    "\n",
    "So from these you can see high and low values, which makes the network learn how to place the attention score on the tokens and thses learn through embeddings. Attention is used to generate tokens, thats how the gpt works.\n",
    "\n",
    "## GPT architecture\n",
    "\n",
    "Basically gpt works same as the transformer architecture but the encoder part is just removed.\n",
    "\n",
    "Also in attention we use scaling in the attention scores so we can protect the model from vanishing gradient problem doesnt attack.\n",
    "\n",
    "Let me explian it onw more time:\n",
    "\n",
    "GPT process :\n",
    "\n",
    "- Tokenized outputs\n",
    "- Embeddings + POS encoding\n",
    "- n Decoders according to the layers\n",
    "    In each decoder the process is :\n",
    "       - Multihead attention\n",
    "           n heads of multihead runs in parallel.\n",
    "            the multihead process is :\n",
    "          - Keys, Query and Value\n",
    "          - Scaled dot product attention (head)\n",
    "                   Basically, in each head the key and query will be done dot product and then its scaled to combat the vanishig gradient after that torch.trill masking for not lookin into the future and then softmax then the value and softmax will matrix multiply. the output will be a blend of inputs vector and attention placed on each token.\n",
    "          - Conconate results\n",
    "          - Linear\n",
    "       - Residual connection (Add) then normalise\n",
    "       - Feed Forward  (Linear -> ReLU -> Linear)\n",
    "       - Residual connection (Add) and normalise\n",
    "- nnLinear\n",
    "- Softmax\n",
    "- Probability sampling generation\n",
    "- Compare targets to outputs -> backpropogate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "learning_rate = 3e-3\n",
    "eval_iters = 250\n",
    "n_embd = 384\n",
    "n_layer = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Book.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s ]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size,(batch_size,))\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad   #this is used to make the computation easier by removing gradients\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()  # to make the model enter the evaluation mode\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits , loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the gpt model we have to do positional encoding to the tokenized inputs, but if we put the positional encoding its a fixed function. which does not learn about the data. so we going to put the positional encoding inside the nn.embedding.\n",
    "\n",
    "self.blocks is the variable containing the sequential running of the decoders of 4 layer using the n_layers\n",
    "LayerNOrm is used to end of the netowrk to make the model to converge better and normalize\n",
    "linear it there so it can make the final outpu workable with the softmax\n",
    "\n",
    "while doing in practice sinisudal embeddings are used to transformer models and while doing GPT they use learnable embeddings\n",
    "\n",
    "im the forawrd pass, we are going to add a code where we are going to create token embedding and positional embedding and add them both and then run blocks on then do layernorm and then do the linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)  # we are creating a embedding table with dimentional of vocab_size and n_embed\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd)  #we are creating embedding table for positional encoding using the block size and n_embed\n",
    "        self.blocks = nn.Sequential(*(Block(n_embd, n_head=n_head) for in range(n_layer))) \n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self, _init_weights)\n",
    "    \n",
    "    def _init_weights(self,module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.positional_embedding_table(torch.arrange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape  # B - Batch, T - Time, C - Channel (size of sequence)\n",
    "            logits = logits.view(B*T,C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)  # we call the forward pass\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)   #we get the probability distribution and dimension is -1 because we want the +1 index expected index\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #takes the highest number of prob\n",
    "            index = torch.cat((index, index_next),dim=1) # concatinates to the next element the whole size\n",
    "        return index\n",
    "    \n",
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device) # this basically the index in the above generate parameter, which is a single zero\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, loss: {losses}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
